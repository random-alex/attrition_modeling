---
title: "Attrition"
author: "Zotov A.V."
date: '23 марта 2018 г '
output: html_document
---
```{r set-options, echo=FALSE, cache=FALSE}
options(width = 300)
```

```{r setup, include=F, echo=FALSE}
require(ggthemes)
require(RColorBrewer)
require(readxl)
require(plotly)
require(zoo)
library(recipes)
require(car)
require(caret)
require(pROC)
require(tidyverse)
require(lime)
require(DALEX)
require(xgboost)
require(GGally)
knitr::opts_chunk$set(echo = TRUE)
dir <- 'data/WA_Fn-UseC_-HR-Employee-Attrition.xlsx'
dir_res <- 'data/data_for_mod.csv'

df <- read_excel(dir) %>% 
  rename(id = EmployeeNumber) %>% 
  #equal for all employers
  select(-c(EmployeeCount,Over18,StandardHours))
col_con <- c("Age", 'DailyRate', 'HourlyRate','MonthlyIncome','MonthlyRate',
             'TotalWorkingYears','DistanceFromHome', 
             'YearsAtCompany', 'YearsInCurrentRole',	'YearsSinceLastPromotion',	'YearsWithCurrManager')

df_def <- read_excel(dir,2) %>% 
  mutate(group = na.locf(group)) %>% 
  separate('value',c('value_old','value_new'),sep = " '") %>% 
  mutate(value_new = str_replace_all(value_new,"'",''))
col_nes <- unique(df_def$group)

df_sum <- df %>% 
  gather(parameter,value,-c(id,Attrition)) 
df_tmp <- df_sum %>% 
  filter(parameter %in% col_nes) %>% 
  left_join(df_def, by = c("parameter" = "group",'value' = 'value_old')) %>% 
  mutate(value = value_new) %>% 
  select(-value_new)

df_sum <- df_sum %>% 
  filter(!(parameter %in% col_nes)) %>% 
  bind_rows(df_tmp)



df_model <- df_sum %>% 
  spread(parameter,value) %>% 
  mutate_at(vars(c(col_con,'PercentSalaryHike','NumCompaniesWorked')),funs(as.numeric))
df_model$Attrition <- as.numeric(as.factor(df_model$Attrition)) - 1

recipe_obj <- df_model %>% 
  recipe(formula = ~ .) %>%
  step_rm(id) %>%
  step_zv(all_predictors()) %>%
  # step_center(col_con) %>%
  # step_scale(col_con) %>%
  step_dummy(all_nominal()) %>% 
  # step_string2factor(Attrition) %>%
  prep(data = df)
recipe_obj

df_model <- bake(recipe_obj, newdata = df_model)

df_model <- df_model %>% 
  mutate_if(is.character,as_factor)
#write data for h2o
#write_csv(df_model, dir_res)

# levels(df_model$JobRole) <- c("HlRep", "HR", "LabTech", "Man", "ManDir", "ResDir", "ResSci", "SalEx", "SalRep")
# levels(df_model$EducationField) <- c("HR", "LSci", "Mar", "Med", "Other", "TechDir")

# split data --------------------------------------------------------------

smp_size <- floor(0.75 * nrow(df_model))

## set the seed to make your partition reproductible
set.seed(123)
train_ind <- sample(seq_len(nrow(df_model)), size = smp_size)

train <- df_model[train_ind, ]
test <- df_model[-train_ind, ]

# modeling ----------------------------------------------------------------

#GLM
mod_glm <- train(as.factor(Attrition) ~ .,
                        family = 'binomial',
                        data = train,
                        method = 'glm')
# summary(mod_glm)
preds <- predict(mod_glm, test)
rocv <- roc(as.numeric(test$Attrition), as.numeric(preds))

#Xgboost

num.folds <- 6
train_mod_matrix <- model.matrix(Attrition ~.-1,
                                 # data = mutate(train,Attrition = as.numeric(Attrition) - 1),
                                 data = train
                                 )
train_xgb <- xgb.DMatrix(data = train_mod_matrix ,
                         # label = as.numeric(train$Attrition) - 1,
                         label = as.numeric(train$Attrition)
                         )

test_mod_matrix <- model.matrix(Attrition ~.-1,
                         # data = mutate(test,Attrition = as.numeric(Attrition) - 1),
                         data = test
                         )
test_xgb <- xgb.DMatrix(data = test_mod_matrix ,
                         # label = as.numeric(test$Attrition) - 1 ,
                        label = as.numeric(test$Attrition)
                        )

sumwpos <- sum((as.numeric(train$Attrition)==1))
sumwneg <- sum(as.numeric(train$Attrition)==0)

param <- list("objective" = "binary:logistic",
              "scale_pos_weight" = sumwneg / sumwpos,
              "bst:eta" = 0.1,
              "bst:max_depth" = 6,
              "eval_metric" = "auc",
              "eval_metric" = "ams@0.15",
              "silent" = 1,
              "nthread" = 16)

mod_xgb <- xgb.cv(param,data = train_xgb,
                  nrounds = 3000,
                  nfold = num.folds,metrics = 'auc')
# mod_xgb$evaluation_log
params <- mod_xgb$params

mod_xgb <- xgb.train(params,data = train_xgb,nrounds = 3000)
# make predictions
preds <- predict(mod_xgb, newdata = test_xgb, type = "prob")
summary(mod_xgb)

preds <- predict(mod_xgb, test_xgb,type = 'class')


# df_sum %>% 
#   filter(parameter %in% col_con ) %>% 
#   spread(parameter,value) %>% 
#   .[,-c(1,2)] %>% 
#   mutate_if(is.character,as.numeric) %>% 
#   summary()
# 
# res <- df_sum %>% 
#   filter(!(parameter %in% c(col_con))) %>% 
#   count(parameter, value) 
```


## Motivation
Here is my attempt to tell you story, that I have received from the data. 
Where is the highest percent of dismissals from work? How does overtime connect with attrition?  Is gender really matter? And many more, if you sit dawn and spend some of your time to read my report)

## Prepare data for exploratory data analysis (EDA)
So, when I usually begin to discover history from data, I spend many hours prepare and cleaning dataset, but not now! Today I have nice, prepared dataset, and need only to do some magic staff in case of better visualization. 


## Ordinal data
Let's begin from ordinal data, just because I want :). Let's see how many attrition in each category of each parameter we have.
```{r ,warning=FALSE, echo=FALSE, fig.height = 44, fig.width = 35, fig.align = "center",out.width = '120%'}
df_sum %>% 
  filter(!(parameter %in% col_con)) %>% 
  ggplot(aes(value,fill = Attrition)) +
  geom_histogram(stat="count",binwidth = 2) +
  facet_wrap(c('parameter'),scales = 'free') +
  labs(x = '',fill = 'Attrition  ') +
  theme_pander(base_size = 35,lp  = 'bottom') +
  scale_fill_brewer(palette = "Paired") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1),
        legend.text=element_text(size=50),
        legend.title = element_text(size=50),legend.box.margin = margin(30,30,30,30)) 

```

So what we see here? We understand, that most of employees travel rarely, work in RnD department, have bachelor or master degree and etc. But there are some more interesting facts. For example, why we see only two category of performance rating? Is it really true that in IBM all employees do their job so well? Also we see that there are unusually a lot of people in OverTime category that quit from work. 
But to tell you the truth,it's a little bit difficult to understand picture due to imbalanced classes… Let’s invite old friend percent to this party!

```{r ,echo=FALSE, fig.height = 44, fig.width = 35, fig.align = "center",out.width = '120%'}
df_sum %>% 
  filter(!(parameter %in% col_con)) %>% 
  group_by(parameter,value) %>% 
  count(Attrition) %>% 
  ungroup() %>% 
  group_by(parameter,value) %>% 
  mutate(prob = n/(n[Attrition == 'No'] + n[Attrition == 'Yes'])) %>% 
  ggplot(aes(value,prob *100, fill = Attrition)) +
  geom_col() +
  labs(y = "Percentage", x = '',fill = 'Attrition  ' ) +
  # labs(x = '',fill = 'Attrition  ') +
  theme_pander(base_size = 35,lp  = 'bottom') +
  facet_wrap(c('parameter'),scales = 'free') +
  scale_fill_brewer(palette = "Paired") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1),
        legend.text=element_text(size=50),
        legend.title = element_text(size=50),legend.box.margin = margin(30,30,30,30)) 

```

Now we can made absolutely logical conclusion that people with frequent business trips,low JobInvolment, low Environment, Job, Relationship  satisfaction and with bad WorkLifeBalance and zero StockOptionLevel more often leave company then the others. One can think that this much trivial conclusion, but I must proof it by data, so I do. What about more specific insights? I have it for you) Look carefully on JobRole category. There are something wrong with Sales Representative. Maybe they have terrible director, or maybe they have to OverTime often. By the way, look at  OverTime section. It is strange, that peoples, who overtime, leave company often, then others. That's maybe mean that people work hard but don't feel any feedback from company.
Now let's see top 5 categories with the biggest attrition.

```{r ,echo=FALSE, fig.height = 14, fig.width = 15, fig.align = "center",out.width = '120%'}

df_sum %>% 
  filter(!(parameter %in% col_con)) %>% 
  group_by(parameter,value) %>% 
  count(Attrition) %>% 
  ungroup() %>% 
  group_by(parameter,value) %>% 
  mutate(prob = n/(n[Attrition == 'No'] + n[Attrition == 'Yes']),
         all = n[Attrition == 'No'] + n[Attrition == 'Yes']) %>% 
  select(-n) %>% 
  spread(Attrition,prob) %>% 
  # filter(Attrition == 'Yes') %>% 
  ungroup() %>% 
  arrange(desc(Yes)) %>% 
  .[1:5,] %>%
  gather(Attrition,prob,c(No,Yes)) %>% 
  mutate(par = as_factor(str_c(parameter,' : \n',value))) %>% 
  ggplot(aes(par,prob,fill = Attrition)) +
  geom_col() +
  geom_label(aes(label = str_c(all * prob )),position = position_stack(vjust = 0.5),size = 7) +
  theme_pander(lp = 'None',base_size = 20) +
  labs(y = "Percentage", x = '') +
  scale_fill_brewer(palette = "Paired") 

```


Here we see,that category with the highest attrition is JobRole:SalesRepresentive with 33 people that leave company vs 50 that stay. Suprising result is that people with high (24%) PercentSalaryHike leave company, but small numbers of data points give me a thought, that this is just fluctuation and nothing more, but we will keep in mind this fact.
Now lets dive a little more dipper in details. Let's see how attrition connect with OverTime and some other parameters

```{r ,echo=FALSE, fig.height = 15, fig.width = 20, fig.align = "center"}
df %>% 
  select(c(Attrition,YearsAtCompany,PercentSalaryHike,MonthlyIncome,OverTime, YearsSinceLastPromotion)) %>% 
  gather(parameter,value,-c(Attrition,YearsAtCompany,OverTime)) %>% 
  ggplot(aes(y = value, x = YearsAtCompany, colour = OverTime)) + 
  geom_jitter(size = 2, alpha = 0.8) + 
  geom_smooth(method = "gam") + 
  facet_wrap(parameter ~ Attrition,labeller = 'label_both',
             scales = 'free',ncol = 2) + 
  theme_pander(base_size = 20,lp = 'bottom',boxes = T) +
  scale_color_brewer(palette = "Paired") +
  theme(legend.text=element_text(size=25),
        legend.title = element_text(size=25),legend.box.margin = margin(10,10,10,10))

```

On each picture on x-axis we have YearsAtCompany for different parameters and different Attrition(on left Attrition = No, on left Attrition = Yes). At the first row two pictures are almost logical, but people,that don't OverTime get more salary on the average.
At the second row we see, that people, that work harder, get less percentSalaryHike, then those who don't overTime on the average. Maybe this is effect of averaging, maybe bad work of HR department. At the last row we see another interesting fact: people, who leave company and work hard, get promotion less often, instead of people, that stay at company. Of course, this is maybe average effect, so CEO IBM shouldn't dismissal all HR department :).
Now let's look at the problem of attrition from other side, from continuous data side.
```{r ,echo=FALSE, fig.height = 12, fig.width = 20, fig.align = "center"}

df_sum %>% 
  filter((parameter %in% col_con)) %>% 
  mutate(value = as.numeric(value)) %>% 
  ggplot(aes(value,fill = Attrition)) +
  geom_density(alpha = 0.7) +
  facet_wrap(c('parameter'),scales = 'free') +
  theme_pander(base_size = 20,lp = 'bottom') +
  scale_fill_brewer(palette = "Paired") +
  labs(x = '') +
  theme(axis.text.x = element_text(angle = 45, hjust = 1),
        legend.text=element_text(size=30),
        legend.title = element_text(size=30),legend.box.margin = margin(10,10,10,10))

```
What we see now? There are some interesting thing - after about five years of work in current role or with current manager number of attrition increase. Of course, there are some trivial conclusions such as older people don't like change work as usual, so percent of attrition smaller when people are older. If you like more digits and pairwise correlation, scroll to the end of report and you will get picture with a lot of of this staff)

So, we are looking in data set a lot and now we a ready for some...

## Modeling
Let's begin with simple and powerful method - logistic regression. I don't want to scare you with formulas and other mathematical staff, just think about it as a black magic box, that can give you a probability of certain class of test data point.
First of all, we need prepare data for modeling. To do so, I will use ```recipes``` package.

Let's see some metrics. Think of AUC as bigger value better, max value = 1
```{r ,echo=F}
cat(str_c('Calculated AUC = ', round(rocv$auc,2),'\nTable of correct and wrong classification \n'))
prediction <- as.numeric(as.numeric(preds) > 0.5)
prop.table(table(test$Attrition,prediction, dnn = c("Actual", "Predicted")),1)


```

Let's now look at variable importance table

```{r , echo=F}
mod_glm_imp <- varImp(mod_glm,scale = T)

mod_glm_imp$importance %>% 
  mutate(Feature = rownames(.)) %>% 
  select(Feature,Overall) %>% 
  arrange(desc(Overall)) %>% 
  mutate(Overall = round(Overall,2)) %>% 
  rename(Importance = Overall) %>% 
  .[1:10,]

```
So as we see from EDA, OverTime is very important feature, and EnvironmentSatisfaction_Low is more important, then JobInvolvement_Low and RelationshipSatisfaction_Low.

Not bad for base model. Maybe we can better? Of course we can. Let's try to invite XGBoost


```{r , echo=F}


```

Look at this metrics:


```{r, echo = F}
prediction <- as.numeric(preds > 0.5)
rocv <- roc(as.numeric(test$Attrition),prediction)
cat(str_c('Calculated AUC = ', round(rocv$auc,2),'\n'))
cat('Table of correct and wrong classification \n')
prop.table(table(test$Attrition, prediction, dnn = c("Actual", "Predicted")),1)

```
Hmmm... What?? Usually XGBoost makes better classification better, then logistic regression, but not today. 
Let's look at importance of variables again and compare it with previous result

```{r, echo=F}

mod_xgb_imp <- xgb.importance(model = mod_xgb,feature_names = colnames(train_xgb))
mod_xgb_imp %>% 
  as.tibble() %>% 
  select(Feature, Gain) %>% 
  rename(Importance = Gain) %>% 
  arrange(desc(Importance)) %>% 
  mutate(Importance = round(Importance,2)) %>% 
  .[1:10,]

```


Again, OverTIme is in top of most important feature. So, if you see, that you researcher again works after eight p.m., don't listen to him, send him to home :)

And time to heavy weapons, time to load H2O.

```{r, echo=F,message=F,warning=F }


require(tidyverse)
require(h2o)
require(lime)

dir <- 'data/data_for_mod.csv'
h2o.init(nthreads = 3)
h2o.no_progress()
seed = 1

# functions ---------------------------------------------------------------


# read and split data -----------------------------------------------------

data <- h2o.importFile(dir)


data$Attrition <- as.factor(data$Attrition)  #encode the binary repsonse as a factor
# h2o.levels(data$Attrition)

splits <- h2o.splitFrame(data = data, 
                         ratios = c(0.7, 0.15),  #partition data into 70%, 15%, 15% chunks
                         seed = seed)  #setting a seed will guarantee reproducibility
train <- splits[[1]]
valid <- splits[[2]]
test <- splits[[3]]

# Identify response and predictor variables
y <- "Attrition"
x <- setdiff(names(data), c(y,'ID'))  #remove the interest rate column because it's correlated with the outcome
# print(x)


# select the values for `alpha` to grid over
  hyper_params <- list( alpha = seq(0, 1,length.out = 20) )
  
  # this example uses cartesian grid search because the search space is small
  # and we want to see the performance of all models. For a larger search space use
  # random grid search instead: {'strategy': "RandomDiscrete"}
  
  # build grid search with previously selected hyperparameters
  # grid <- h2o.grid(x = x, y = y, 
  #                  training_frame = train, 
  #                  validation_frame = valid,
  #                  algorithm = "glm",
  #                  nfolds = 3,
  #                  seed = seed,
  #                  lambda_search = T,
  #                  balance_classes = T,
  #                  family = 'binomial', 
  #                  grid_id = "glm_fit1_grid13", 
  #                  hyper_params = hyper_params,
  #                  search_criteria = list(strategy = "Cartesian"))
  # 
  # # Sort the grid models by AUC
  # sortedGrid <- h2o.getGrid("glm_fit1_grid11", sort_by = "auc", decreasing = T)
  
  
```
Connection successful, great. Let's again begin with logistic regression but with grid search for best fitting parameters

```{r, echo=F}
glm_fit_tuned <- h2o.glm(x = x, 
                           y = y, 
                           training_frame = train,
                           model_id = "glm_fit_tuned",
                           validation_frame = valid,
                           nfolds = 3,
                           seed = seed,
                           alpha = 0.075,
                           lambda_search = T,
                           balance_classes = T,
                           family = "binomial")  #similar to R's glm, h2o.glm has the family argument

```

To tell you the truth, I love H2O. This is powerful and comfortable framework for ML.
Let's look at some metrics

```{r, echo=F}
  glm_predict <- h2o.performance(glm_fit_tuned,test)
  cat(str_c('Calculated AUC = ', round(h2o.auc(glm_predict),3),'\n'))
  h2o.confusionMatrix(glm_predict)

```
Wow!! Very nice AUC for test dataset! Now time to look at variable importance:



```{r,echo=F}
h2o.varimp(glm_fit_tuned) %>% 
  as.tibble() %>% 
  rename(Feature = names, Importance = coefficients) %>% 
  .[1:10,]

```
So, now it's time to make some general conclusions. First of all, do something with Sales representative, they leave company too often. Secondly, look carefully after workaholics with OverTime, maybe you don't promote or increase salary as well as they work. Thirdly, don't forget about workers, that travel frequently, they tend to be tired and leave company, probably it will be good idea to give one day vacation after business trip. And of course, don't forget to work with such trivial, but important things as Environment Satisfaction,Job Satisfaction. 


## P.s. Promised picture with a lot of pairwise correlation

Picture with as much information about numerical data as I could paste in one box
```{r ,echo=FALSE,, fig.height = 16, fig.width = 20, fig.align = "center"}
  pp <- df_sum %>% 
  filter((parameter %in% col_con)) %>% 
  mutate(value = as.numeric(value)) %>% 
  spread(parameter,value) %>% 
  select(-id) %>% 
  ggpairs(aes(col = Attrition,alpha = 0.4),
          columns = c(2:12),
          lower = list(continuous = "smooth"), 
          cardinality_threshold = 60 ) +
  theme_pander()
for(i in 1:pp$nrow) {
  for(j in 1:pp$ncol){
    pp[i,j] <- pp[i,j] + 
      scale_fill_brewer(palette = "Paired") +
      scale_color_brewer(palette = "Paired")
  }
}
  
print(pp)

```

